{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQq4mAoKnTws5kmWhPMj8n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tesdem/Examples/blob/master/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jXO-p4y7LPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import main data analysis libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# note we use scipy for generating a uniform distribution in the model optimization step\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# note that because of the different dataset and algorithms, we use different sklearn libraries from Day 1 \n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# hide warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdev3r3W7LPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we load the dataset and save it as the variable data\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# if we want to know what sort of detail is provided with this dataset, we can call .keys()\n",
        "data.keys()\n",
        "\n",
        "# the info at the .DESCR key will tell us more \n",
        "print (data.DESCR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yhl7zIo7LPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Analyze the Data\n",
        "X = pd.DataFrame(data.data, columns = data.feature_names)\n",
        "\n",
        "# we can then look at the top of the dataframe to see the sort of values it contains\n",
        "X.describe(include = 'all')\n",
        "\n",
        "# we can now look at our target variable \n",
        "y = data.target\n",
        "\n",
        "# we can see that it is a list of 0s and 1s, with 1s matching to the Benign class y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_BoY1jQ7LP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can analyse the data in more detail by understanding how the features and target variables interact\n",
        "# we can do this by grouping the features and the target into the same dataframe\n",
        "# note we create a copy of the data here so that any changes don't impact the original data\n",
        "\n",
        "full_dataset = X.copy()\n",
        "full_dataset['target'] = y.copy()\n",
        "\n",
        "# let's take a look at the first few lines of the dataset\n",
        "full_dataset.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBw7Mnf_7LQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets see how balanced the classes are (and if that matches to our expectation)\n",
        "full_dataset['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdzD9RxS7LQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's evaluate visually how well our classes are differentiable on the pairplots\n",
        "# can see two classes being present on a two variables charts?\n",
        "# the pairplot function is an excellent way of seeing how variables inter-relate, but 30 feature can make studying each combination difficult!\n",
        "sns.pairplot(full_dataset, hue = 'target')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hxXYzm07LQg",
        "colab_type": "text"
      },
      "source": [
        "* We can clearly see the presence of two clouds with different colors, representing our target classes. \n",
        "* Of course, they are still mixed to some extent, but if we were to visualise the variables in multi-dimentional space they would become more separable.\n",
        "* Now let's check the Pearson's correlation between pairs of our features and also between the features and our target.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUv8AZA57LQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can again use seaborn to easily create a visually interesting chart \n",
        "plt.figure(figsize = (15, 10))\n",
        "\n",
        "# we can add the annot=True parameter to the sns.heatmap arguments if we want to show the correlation values \n",
        "sns.heatmap(full_dataset.corr(method='pearson'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIGgI18R7LQz",
        "colab_type": "text"
      },
      "source": [
        "* Dark red colours are positilvey correlated with the corresponding feature, dark blue features are negatively correlated.\n",
        "* We can see that some values are negatively correlated with our target variable.\n",
        "* This information could help us with feature engineering.\n",
        "\n",
        "\n",
        "**Split data**\n",
        "* In order to train our model and see how well it performs, we need to split our data into training and testing sets.\n",
        "* We can then train our model on the training set, and test how well it has generalised to the data on the test set.\n",
        "* There are a number of options for how we can split the data, and for what proportion of our original data we set aside for the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoYZziEo7LQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Because our classes are not absolutely equal in number, we can apply stratification to the split \n",
        "# and be sure that the ratio of the classes in both train and test will be the same \n",
        "# you can learn about the other parameters by looking at the documentation \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC3d9kLD7LRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# as with Day 1, we can get shape of test and training sets\n",
        "print('Training Set:')\n",
        "print('Number of datapoints: ', X_train.shape[0])\n",
        "print('Number of features: ', X_train.shape[1])\n",
        "print('\\n')\n",
        "print('Test Set:')\n",
        "print('Number of datapoints: ', X_test.shape[0])\n",
        "print('Number of features: ', X_test.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0bt7U8H7LRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and we can verify the stratifications using np.bincount\n",
        "print('Labels counts in y:', np.bincount(y))\n",
        "print('Percentage of class zeroes in class_y',np.round(np.bincount(y)[0]/len(y)*100))\n",
        "\n",
        "print(\"\\n\")\n",
        "print('Labels counts in y_train:', np.bincount(y_train))\n",
        "print('Percentage of class zeroes in y_train',np.round(np.bincount(y_train)[0]/len(y_train)*100))\n",
        "\n",
        "print(\"\\n\")\n",
        "print('Labels counts in y_test:', np.bincount(y_test))\n",
        "print('Percentage of class zeroes in y_test',np.round(np.bincount(y_test)[0]/len(y_test)*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yEJRDoC7LSH",
        "colab_type": "text"
      },
      "source": [
        "### Choose a Baseline algorithm\n",
        "* Building a model in `sklearn` involves:\n",
        "* we can create a baseline model to benchmark our other estimators against\n",
        "* this can be a simple estimator or we can use a dummy estimator to make predictions in a random manner \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up8bReTu7LSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this creates our dummy classifier, and the value we pass in to the strategy parameter dtermn\n",
        "dummy = DummyClassifier(strategy='uniform', random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ6HbhjC7LSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train and Test the Model\n",
        "# \"Train\" model\n",
        "dummy.fit(X_train, y_train)\n",
        "\n",
        "# from this, we can generate a set of predictions on our unseen features, X_test\n",
        "dummy_predictions = dummy.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd_OhKeL7LSc",
        "colab_type": "text"
      },
      "source": [
        "### Choose an evaluation metric\n",
        "* We then need to compare these predictions with the actual result and measure them in some way. This is where the selection of evaluation metric is important. \n",
        "* Classification metrics include:\n",
        "  * `accuracy`: this assess how often the model selects the best class. This can be more useful when there are balanced classes (i.e. there are a similar number of instances of each class we are trying to predict). \n",
        "    * There are some limits to this metric. For example, if we think about something like credit card fraud, where the instances of fraudulent transactions might be 0.5%, then a model that *always* predicts that a transaction is not fraudulent will be 99.5% accurate! So we often need metrics that can tell us how a model performs in more detail. \n",
        "  * `f1 score`: \n",
        "  * `roc_auc`: \n",
        "  * `recall`: \n",
        "  * We recommend you research these metrics to improve your understanding of how they work. Try to look up an explanation or two (for example on wikipedia and scikit-learn documentation) and write a one line summary in the space provided above. Then, below, when we implement a scoring function, select these different metrics and try to explain what is happening. This will help cement you knowledge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G4skkVw7LSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(y_test, y_pred):\n",
        "    # this block of code returns all the metrics we are interested in \n",
        "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "    f1 = metrics.f1_score(y_test, y_pred)\n",
        "    auc = metrics.roc_auc_score(y_test, y_pred)\n",
        "\n",
        "    print (\"Accuracy\", accuracy)\n",
        "    print ('F1 score: ', f1)\n",
        "    print ('ROC_AUC: ' , auc)\n",
        "\n",
        "# we can call the function on the actual results versus the predictions\n",
        "# we will see that the metrics are what we'd expect from a random model \n",
        "evaluate(y_test, dummy_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hfEyfPgOUUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwH1N05k7LTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test Alternative Models\n",
        "\n",
        "## here we fit a new estimator and use cross_val_score to get a score based on a defined metric \n",
        "\n",
        "# instantiate logistic regression classifier\n",
        "log_clf = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AwLBtFZy6kM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41gWjnCI7LTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we pass our estimator and data to the method. we also specify the number of folds (default is 3)\n",
        "# the default scoring method is the one associated with the estimator we pass in\n",
        "# we can use the scoring parameter to pass in different scoring methods. Here we use f1.  \n",
        "cross_val_score(log_clf, X, y, cv=5, scoring=\"f1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GzesuUl7LUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can see that this returns a score for all the five folds of the cross_validation\n",
        "# if we want to return a mean, we can store as a variable and calculate the mean, or do it directly on the function\n",
        "# this time we will use accuracy\n",
        "cross_val_score(log_clf, X, y, cv=5, scoring=\"accuracy\").mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK9JJFie7LUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets do this again with a different model\n",
        "rnd_clf = RandomForestClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm6nrHVo7LVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and pass that in \n",
        "cross_val_score(rnd_clf, X, y, cv=5, scoring=\"accuracy\").mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMj90Zma7LVW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Ensemble models** \n",
        "\n",
        "* Let's take this opportunity to explore ensemble methods.\n",
        "* The goal of ensemble methods is to combine different classifiers into a meta-classifier that has better generalization performance than each individual classifier alone. \n",
        "* There are several different approaches to achieve this, including **majority voting** ensemble methods, which we select the class label that has been predicted by the majority of classifiers.\n",
        "* The ensemble can be built from different classification algorithms, such as decision trees, support vector machines, logistic regression classifiers, and so on. Alternatively, we can also use the same base classification algorithm, fitting different subsets of the training set. \n",
        "* Indeed, Majority voting will work best if the classifiers used are different from each other and/or trained on different datasets (or subsets of the same data) in order for their errors to be uncorrelated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXoEZvSB7LVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets instantiate an additional model to make an ensemble of three models\n",
        "dt_clf = DecisionTreeClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EXvS7LB7LVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and an ensemble of them\n",
        "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('dc', dt_clf)],                              \n",
        "                              # here we select soft voting, which returns the argmax of the sum of predicted probabilities\n",
        "                              voting='soft')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy6ufkP27LVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# here we can cycle through the individual estimators \n",
        "# for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "\n",
        "for clf in (log_clf, rnd_clf, dt_clf, voting_clf):\n",
        "    \n",
        "    # fit them to the training data \n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    # get a prediction\n",
        "    y_pred = clf.predict(X_test)\n",
        "    \n",
        "    # and print the prediction \n",
        "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2zu93937LWA",
        "colab_type": "text"
      },
      "source": [
        "* We can see that `voting classifier` in this the case does have a slight edge on the other models (note that this could vary depending on how the data is split at training time).\n",
        "* This is an interesting approach and one to consider when you are developing your models.\n",
        "\n",
        "### Step 9: Choose the best model and optimise its parameters\n",
        "* We can see that we have improved our model as we have added features and trained new models.\n",
        "* At the point that we feel comfortable with a good model, we can start to tune the parameters of the model.\n",
        "* There are a number of ways to do this. We applied `GridSearchCV` to identify the best hyperparameters for our models on Day 1.\n",
        "* There are other methods available to use that don't take the brute force approach of `GridSearchCV`.\n",
        "* We will cover an implementation of `RandomizedSearchCV` below, and use the exercise for you to implement it on the other datatset.\n",
        "* We use this method to search over defined hyperparameters, like `GridSearchCV`, however a fixed number of parameters are sampled, as defined by `n_iter` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHbewfz-7LWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will optimise logistics regression \n",
        "# we can create hyperparameters as a list, as in type regularization penalty \n",
        "penalty = ['l1', 'l2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9rvMkfV7LWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# or as a distribution of values to sample from -'C' is the hyperparameter controlling the size of the regularisation penelty \n",
        "C = uniform(loc=0, scale=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra_NBAx-7LWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we need to pass these parameters as a dictionary of {param_name: values}\n",
        "hyperparameters = dict(C=C, penalty=penalty)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI8x1K4i7LWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we instantiate our model\n",
        "randomizedsearch = RandomizedSearchCV(log_clf, hyperparameters, random_state=1, \n",
        "                                      n_iter=100, cv=5, verbose=0, n_jobs=-1)\n",
        "\n",
        "# and fit it to the data \n",
        "best_model = randomizedsearch.fit(X, y)\n",
        "\n",
        "# from this, we can generate a set of predictions on our unseen features, X_test\n",
        "best_predictions = best_model.predict(X_test)\n",
        "\n",
        "# and evaluate model performance \n",
        "evaluate(y_test, best_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaLMK0HB7LWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# and we can call this method to return the best parameters the search returned\n",
        "best_model.best_estimator_\n",
        "\n",
        "# and - we can evaluate the model using the cross validation method discussed above\n",
        "cross_val_score(best_model, X, y, cv=5, scoring=\"accuracy\").mean()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}